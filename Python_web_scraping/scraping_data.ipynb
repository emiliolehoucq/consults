{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1384d6f3",
   "metadata": {},
   "source": [
    "# Scrape job postings from [CESNET-L](https://www.cesnet-l.net/)\n",
    "\n",
    "Emilio Lehoucq - 3/4/24\n",
    "\n",
    "Note: this is not the most optimal code, but it solves the problem in a reasonable way.\n",
    "\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "077fd100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from pandas import DataFrame\n",
    "from pandas import read_csv\n",
    "from pandas import concat\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d392b73",
   "metadata": {},
   "source": [
    "## Define custom functions for this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a481e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm not turning the first three functions into a class with three methods because it doesn't work within\n",
    "# soup_compilation.find_all('a', href = True, string = Class.method)\n",
    "def contains_posting(text):\n",
    "    '''\n",
    "    Function to filter HTML <a> elements containing postings.\n",
    "\n",
    "    Input: text in an <a> element.\n",
    "    Outupt: boolean--True if contains posting, False otherwise.\n",
    "    '''\n",
    "    return text and ('faculty position' in text.lower() or 'professor' in text.lower())\n",
    "\n",
    "def contains_plain_text(text):\n",
    "    '''\n",
    "    Function to filter HTML <a> elements containing 'text/plain'.\n",
    "\n",
    "    Input: text in an <a> element.\n",
    "    Outupt: boolean--True if contains 'text/plain', False otherwise.\n",
    "    '''\n",
    "    return text and ('text/plain' in text.lower())\n",
    "\n",
    "def contains_html(text):\n",
    "    '''\n",
    "    Function to filter HTML <a> elements containing 'text/html'.\n",
    "\n",
    "    Input: text in an <a> element.\n",
    "    Outupt: boolean--True if contains 'text/html', False otherwise.\n",
    "    '''\n",
    "    return text and ('text/html' in text.lower())\n",
    "\n",
    "def get_compilations(web_driver):\n",
    "    '''\n",
    "    Function to log into Kent State University listserv, go to CESNET-L, and find all HTML <li> elements.\n",
    "    The <li> elements contain the hyperlinks to all previous weekly compilations of messages in the listserv.\n",
    "    \n",
    "    Input: web driver.\n",
    "    Output: list of selenium.webdriver.remote.webelement.WebElement.\n",
    "    \n",
    "    Dependencies: selenium webdriver.\n",
    "    \n",
    "    Note: I created this function because otherwise looping over <li> elements I got a stale element error.\n",
    "    There's probably a better way to fix that error, but this solves the problem and works in this context.\n",
    "    '''\n",
    "    # Define https://listserv.kent.edu/ credentials\n",
    "    email = \"your_email\"\n",
    "    password = \"your_password\"\n",
    "\n",
    "    # Go to log in page \n",
    "    url_login = \"https://listserv.kent.edu/cgi-bin/wa.exe?LOGON\"\n",
    "    web_driver.get(url_login)\n",
    "\n",
    "    # Find email field and send the email to the input field\n",
    "    web_driver.find_element(\"id\", \"Email Address\").send_keys(email)\n",
    "\n",
    "    # Find password input field and insert password\n",
    "    web_driver.find_element(\"id\", \"Password\").send_keys(password)\n",
    "\n",
    "    # Click log in button\n",
    "    web_driver.find_element(\"name\", \"e\").click()\n",
    "\n",
    "    # Find the CESNET-L listserv and click on it to get to the archive\n",
    "    cesnet_archive_element = web_driver.find_element(By.LINK_TEXT, \"CESNET-L\")\n",
    "    cesnet_archive_element.click()\n",
    "\n",
    "    # Find all <li> elements\n",
    "    li_elements = web_driver.find_elements(By.CSS_SELECTOR, \"li\")\n",
    "    \n",
    "    # Return all selenium.webdriver.remote.webelement.WebElement\n",
    "    return li_elements\n",
    "\n",
    "def get_message_data(web_driver, url_message, dictionary, beginning_key):\n",
    "    '''\n",
    "    Function to go to URL of message, add source code to dictionary, parse page source and add\n",
    "    BeautifulSoup object to dictionary, and get text and add to dictionary.\n",
    "\n",
    "    Inputs:\n",
    "        web_driver: web driver.\n",
    "        url_message: url (string).\n",
    "        dictionary: dictionary to store data.\n",
    "        beginning_key: string that should be at the beginning of each of the keys in the dictionary.\n",
    "    Output: none.\n",
    "\n",
    "    Dependencies: selenium webdriver, BeautifulSoup.\n",
    "    '''\n",
    "    # Go to the URL of the message\n",
    "    web_driver.get(url_message)\n",
    "    # Adding sleep time before getting source code\n",
    "    sleep(10)\n",
    "    # Add source code to dictionary\n",
    "    dictionary[beginning_key + '_message_source_code'] = web_driver.page_source\n",
    "    # Parse page source\n",
    "    soup_message = BeautifulSoup(web_driver.page_source)\n",
    "    # Add BeautifulSoup object to dictionary\n",
    "    dictionary[beginning_key + '_message_soup'] = soup_message\n",
    "    # Get text and add to dictionary\n",
    "    dictionary[beginning_key + '_message_text'] = soup_message.get_text()\n",
    "    \n",
    "def populate_none(dictionary, beginning_key):  \n",
    "    '''\n",
    "    Function to populate missing data to dictionary.\n",
    "    \n",
    "    Inputs:\n",
    "        dictionary: dictionary to populate.\n",
    "        beginning_key: string that should be at the beginning of each of the keys in the dictionary.\n",
    "    Outputs: none.\n",
    "    '''\n",
    "    dictionary[beginning_key + '_message_source_code'] = None\n",
    "    dictionary[beginning_key + '_message_soup'] = None\n",
    "    dictionary[beginning_key + '_message_text'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4a92f6",
   "metadata": {},
   "source": [
    "## Get current number of weekly compilations\n",
    "\n",
    "Doing this because number changes every week, so it's worth even if only working on this script on two or three separate weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd9a9353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Get current number of weekly compilations in CESNET-L archive\n",
    "number_compilations = len(get_compilations(driver))\n",
    "\n",
    "# Quit driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd97dd8",
   "metadata": {},
   "source": [
    "## Collect postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7469075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over current number of weekly compilations\n",
    "for i in range(number_compilations):\n",
    "# # This way I can test my code across compilations in different years in case structure changes\n",
    "# for i in range(1, number_compilations, 50):\n",
    "    # Create list to store the data for current weekly compilation\n",
    "    data_weekly_compilation = []\n",
    "    # Initialize Chrome driver\n",
    "    driver = webdriver.Chrome()\n",
    "    # Get compilations\n",
    "    weekly_compilations = get_compilations(driver)\n",
    "    # Get current weekly compilation\n",
    "    compilation = weekly_compilations[i]\n",
    "    # Get the week of the compilation\n",
    "    week_compilation = compilation.text\n",
    "    # Go to compilation\n",
    "    driver.get(compilation.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\"))\n",
    "    # Adding sleep time before getting source code\n",
    "    sleep(10)\n",
    "    # For the next two steps, I'm sure there's a way to do it with selenium without having to \n",
    "    # get the source code and parse it. But this gets the job done and came to mind first\n",
    "    # Get and parse source code of compilation\n",
    "    soup_compilation = BeautifulSoup(driver.page_source)\n",
    "    # Find URLs for postings\n",
    "    url_postings = [a_element['href'] for a_element in soup_compilation.find_all('a', href = True, string = contains_posting) if 'https' in a_element['href']]\n",
    "    # Iterate over each posting\n",
    "    for url in url_postings:\n",
    "        # Create dictionary to store posting data\n",
    "        posting_data = {}\n",
    "        # Add current timestamp to dictionary\n",
    "        posting_data['timestamp_collection'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        # Add week of the compilation to dictionary\n",
    "        posting_data['week_compilation'] = week_compilation\n",
    "        # Go to the URL of the posting\n",
    "        driver.get(url)\n",
    "        # Adding sleep time before getting source code\n",
    "        sleep(10)\n",
    "        # Again, the next two steps can probably be done differently\n",
    "        # Get and parse source code\n",
    "        soup_posting = BeautifulSoup(driver.page_source)\n",
    "        # Try to get data for messages\n",
    "        # Doing error handling because some messages have only 'text/plain', others only 'text/html', \n",
    "        # and others both (maybe some neither?)  \n",
    "        # 'text/plain' and 'text/html' seem to give the same results. Could keep only one, but better safe than sorry\n",
    "        # Define URL base\n",
    "        url_base = 'https://listserv.kent.edu'\n",
    "        # Try to get 'text/plain'\n",
    "        key_base = 'plain_text'\n",
    "        try:\n",
    "            # Find the URL of the plain text message\n",
    "            url_plain_text = url_base + soup_posting.find('a', href = True, string = contains_plain_text)['href']\n",
    "            # Get message data\n",
    "            get_message_data(driver, url_plain_text, posting_data, key_base)\n",
    "        except:\n",
    "            populate_none(posting_data, key_base)\n",
    "        # Try to get 'text/html'\n",
    "        key_base = 'html'\n",
    "        try:\n",
    "            # Find the URL of the html message\n",
    "            url_html = url_base + soup_posting.find('a', href = True, string = contains_html)['href']\n",
    "            # Get message data\n",
    "            get_message_data(driver, url_plain_text, posting_data, key_base)\n",
    "        except:\n",
    "            populate_none(posting_data, key_base)\n",
    "        # Appending posting data dictionary to list for current weekly compilation\n",
    "        data_weekly_compilation.append(posting_data)\n",
    "        # Wait before going to next posting\n",
    "        sleep(10)\n",
    "    # Convert list for current weekly compilation to data frame\n",
    "    data_weekly_compilation_df = DataFrame(data_weekly_compilation)\n",
    "    # Save data frame to csv. Saving for each compilation in case something happens, I can restart there\n",
    "    data_weekly_compilation_df.to_csv('data_compilation_' + week_compilation.replace(\",\", \"\").replace(\" \", \"_\").lower() + '.csv', index = False)\n",
    "    # Quit driver\n",
    "    driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
